{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorials table of content:\n",
    "\n",
    "- [Tutorial 1: Run a first scenario](./Tutorial-1_Run_your_first_scenario.ipynb)\n",
    "- [Tutorial 2: Add contributivity measurements methods](./Tutorial-2_Add_contributivity_measurement.ipynb)\n",
    "- Tutorial 3: Use a custom dataset\n",
    "\n",
    "\n",
    "# Tutorial 3 : Use homemade dataset \n",
    "\n",
    "With this example, we dive deeper into the potential of the library, and run a scenario on a new dataset, that we will implement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Prerequisites\n",
    "\n",
    "In order to run this example, you'll need to:\n",
    "\n",
    "* use python 3.7 +\n",
    "* install this package https://pypi.org/project/mplc/\n",
    "\n",
    "If you did not follow our firsts tutorials, it is highly recommended to [take a look at it !](https://github.com/SubstraFoundation/distributed-learning-contributivity/tree/master/notebooks/examples/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install mplc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Context \n",
    "\n",
    "In collaborative data science projects partners sometimes need to train a model on multiple datasets, contributed by different data providing partners. In such cases the partners might have to measure how much each dataset involved contributed to the performance of the model. This is useful for example as a basis to agree on how to share the reward of the ML challenge or the future revenues derived from the predictive model, or to detect possible corrupted datasets or partners not playing by the rules. The library explores this question and the opportunity to implement some mechanisms helping partners in such scenarios to measure each dataset's *contributivity* (as *contribution to the performance of the model*).\n",
    "\n",
    "In the [first tutorial](./Tutorial-1_Run_your_first_scenario.ipynb), you learned how to parametrize and run a scenario.\n",
    "In the [second tutorial](./Tutorial-2_Add_contributivity_measurement.ipynb), you discovered how to add to your scenario run one of the contributivity measurement methods available.\n",
    "In this third tutorial, we are going to use a custom dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjxjesO3fyLN"
   },
   "source": [
    "### The dataset : Sentiment140\n",
    "We are going to use a subset of the [sentiment140](http://help.sentiment140.com/for-students) dataset and try to \n",
    "classified short film review, between positive sentiments and negative sentiments for movies. \n",
    "\n",
    "*The whole machine learning process is inspired from this [article](https://medium.com/@alyafey22/sentiment-classification-from-keras-to-the-browser-7eda0d87cdc6)*\n",
    "Please note that the library provided a really easy way to adapt a single partner, common machine learning use case with tensorflow, to a multipartner case, with contributivity measurement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "LDhMiOgNfyLO",
    "outputId": "24d31bb5-ea34-4625-c292-a9704c60e0c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 11s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Embedding\n",
    "\n",
    "from mplc.dataset import Dataset\n",
    "from mplc.scenario import Scenario\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n17Q_qx6fyLR"
   },
   "source": [
    "## 3 - Generation, and preparation of the dataset\n",
    " \n",
    "The scenario object needs a dataset object to run. In the previous tutorials, we indicate which one to generate automatically by passing a name of a pre-implemented dataset to the scenario constructor. \n",
    "Here, we will create this dataset object and pass it to the scenario constructor. To do so, we are going to create a new class, which inherit from the mplc.Dataset abstract class.\n",
    "\n",
    "A sub-class of Dataset needs few attribute and method. First, the constructor of the Dataset object needs few arguments.\n",
    "### Dataset generator :\n",
    "\n",
    "The structure of the dataset generator is represented below:\n",
    "\n",
    "```python\n",
    "dataset = Dataset(\n",
    "    \"name\",\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    input_shape,\n",
    "    num_classes,\n",
    ")\n",
    "```\n",
    "#### Data labels\n",
    "The data labels can take whatever shape you need, with only one condition. \n",
    "The labels need to be convertible into string format, and with respect to the condition that if label1 is equal to label2 (\n",
    "reciprocally different from), therefore str(label1) must be equal to str(label2) (reciprocally different from)\n",
    "#### Model generator\n",
    "This method needs to be implemented, and provides the model use, which will be trained by the `Scenario` object.\n",
    "Note: It is mandatory to have loss and accuracy as metrics for your model.\n",
    "\n",
    "#### Train/validation/test splits\n",
    "\n",
    "The `Dataset` constructor (called via `super()`)  must be provided some separated train and test sets (referred to as global train set and global test set).\n",
    "The global train set is then further split into a global train set and a global validation set, by the function `train_val_split_global`. Please denote that if this function is not overwritten, the sklearn's `train_test_split` function will be called by default, and 10% of the training set will be use as validation set.\n",
    "In the multi-partner learning computations, the global validation set is used for early stopping and the global test set is used for performance evaluation.\n",
    "The global train set is then split amongst partners (according to the scenario configuration) to populate the partner's local datasets.\n",
    "For each partner, the local dataset will be split into separated train, validation and test sets, using the `train_test_split_local` and `train_val_split_local` methods.\n",
    "These are not mandatory, by default the local dataset will not be split. \n",
    "Denote that currently, the local validation and test set are not used, but they are available for further developments of multi-partner learning and contributivity measurement approaches.\n",
    "\n",
    "### Dataset construction\n",
    "Now that we know all of that, we can create our dataset class.\n",
    "#### Download and unzip data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "_7RrpjRnfyLS",
    "outputId": "5fdfed8b-c223-4742-f4e6-0fc86108de7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 77.5M  100 77.5M    0     0  15.0M      0  0:00:05  0:00:05 --:--:-- 15.9M\n",
      "Archive:  trainingandtestdata.zip\n",
      "  inflating: testdata.manual.2009.06.14.csv  \n",
      "  inflating: training.1600000.processed.noemoticon.csv  \n"
     ]
    }
   ],
   "source": [
    "!curl https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip --output trainingandtestdata.zip\n",
    "!unzip trainingandtestdata.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qf36Veh3fyLU"
   },
   "source": [
    "#### Define our Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TGfN4EvfyLU",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Sentiment140(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        x, y = self.load_data()\n",
    "        self.max_tokens = self.getMax(x)\n",
    "\n",
    "        self.num_words = None\n",
    "        self.word_index = self.tokenize()\n",
    "        self.num_words = len(self.word_index)\n",
    "\n",
    "        x = self.create_sequences(x)\n",
    "        y = self.preprocess_dataset_labels(y)\n",
    "\n",
    "        self.input_shape = self.max_tokens\n",
    "        self.num_classes = len(np.unique(y))\n",
    "\n",
    "\n",
    "        print('length of the dictionary ',len(self.word_index))\n",
    "        print('max token ', self.max_tokens)\n",
    "        print('num classes', self.num_classes)\n",
    "\n",
    "        (x_train, x_test) = train_test_split(x, shuffle = False)\n",
    "        (y_train, y_test) = train_test_split(y, shuffle = False)\n",
    "\n",
    "        super(Sentiment140, self).__init__(dataset_name='sentiment140',\n",
    "                                           num_classes=self.num_classes,\n",
    "                                           input_shape=self.input_shape,\n",
    "                                           x_train=x_train,\n",
    "                                           y_train=y_train,\n",
    "                                           x_test=x_test,\n",
    "                                           y_test=y_test)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(): #  load the data, transform the .csv into usable dataframe\n",
    "\n",
    "        df_train = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding = \"raw_unicode_escape\", header=None)\n",
    "        df_test = pd.read_csv(\"testdata.manual.2009.06.14.csv\", encoding = \"raw_unicode_escape\",  header=None)\n",
    "\n",
    "        df_train.columns = [\"polarity\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "        df_test.columns = [\"polarity\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "\n",
    "        # We keep only a fraction of the whole dataset\n",
    "\n",
    "        df_train = df_train.sample(frac = 0.1)\n",
    "\n",
    "        x = df_train[\"text\"]\n",
    "        y = df_train[\"polarity\"]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    #  Preprocessing methods\n",
    "    @staticmethod\n",
    "    def process( txt):\n",
    "        out = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "        out = out.split()\n",
    "        out = [word.lower() for word in out]\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def getMax( data):\n",
    "        max_tokens = 0\n",
    "        for txt in data:\n",
    "            if max_tokens < len(txt.split()):\n",
    "                max_tokens = len(txt.split())\n",
    "        return max_tokens\n",
    "\n",
    "\n",
    "    def tokenize(self, thresh = 5):\n",
    "        count  = dict()\n",
    "        idx = 1\n",
    "        word_index = dict()\n",
    "        for txt in x:\n",
    "            words = self.process(txt)\n",
    "            for word in words:\n",
    "                if word in count.keys():\n",
    "                    count[word] += 1\n",
    "                else:\n",
    "                    count[word]  = 1\n",
    "        most_counts = [word for word in count.keys() if count[word]>=thresh]\n",
    "        for word in most_counts:\n",
    "            word_index[word] = idx\n",
    "            idx+=1\n",
    "        return word_index\n",
    "\n",
    "\n",
    "    def create_sequences(self,data):\n",
    "        tokens = []\n",
    "        for txt in data:\n",
    "            words = self.process(txt)\n",
    "            seq = [0] * self.max_tokens\n",
    "            i = 0\n",
    "            for word in words:\n",
    "                start = self.max_tokens-len(words)\n",
    "                if word.lower() in self.word_index.keys():\n",
    "                    seq[i+start] = self.word_index[word]\n",
    "                i+=1\n",
    "            tokens.append(seq)\n",
    "        return np.array(tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_dataset_labels( label):\n",
    "        label = np.array([e/4 for e in label])\n",
    "        return label\n",
    "\n",
    "\n",
    "    def generate_new_model(self): # Define the model generator\n",
    "        model = Sequential()\n",
    "        embedding_size = 8\n",
    "        model.add(Embedding(input_dim=self.num_words,\n",
    "                        output_dim=embedding_size,\n",
    "                        input_length=self.max_tokens,\n",
    "                        name='layer_embedding'))\n",
    "\n",
    "        model.add(GRU(units=16, name = \"gru_1\",return_sequences=True))\n",
    "        model.add(GRU(units=8, name = \"gru_2\" ,return_sequences=True))\n",
    "        model.add(GRU(units=4, name= \"gru_3\"))\n",
    "        model.add(Dense(1, activation='sigmoid',name=\"dense_1\"))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=\"Adam\",\n",
    "                  metrics=['accuracy'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ngCAllxOfyLW"
   },
   "source": [
    "#### Create dataset\n",
    "\n",
    "And we can eventually generate our object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gh0VIIzhfyLj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_dataset = Sentiment140()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4 - Create the custom scenario\n",
    "The dataset can be passed to the scenario, through the `dataset` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mplc.scenario import Scenario\n",
    "\n",
    "my_scenario = Scenario(3, [0.1, 0.4, 0.5], dataset=my_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it!\n",
    "\n",
    "Now you can explore our other tutorials for a better overview of what can be done with `mplc`!\n",
    "\n",
    "This work is collaborative, enthusiasts are welcome to comment open issues and PRs or open new ones.\n",
    "\n",
    "Should you be interested in this open effort and would like to share any question, suggestion or input, you can use the following channels:\n",
    "\n",
    "- This Github repository (issues or PRs)\n",
    "- Substra Foundation's [Slack workspace](https://substra-workspace.slack.com/join/shared_invite/zt-cpyedcab-FHYgpy08efKJ2FCadE2yCA), channel `#workgroup-mpl-contributivity`\n",
    "- Email: hello@substra.org\n",
    "\n",
    "![logo Substra Foundation](../../img/substra_logo_couleur_rvb_w150px.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "2 _Sentiment140.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
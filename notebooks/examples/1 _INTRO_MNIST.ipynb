{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1H1pfmz1ddPG"
   },
   "source": [
    "# MNIST \n",
    "\n",
    "This example is an implementation of federated learning using Substra's Distributed Learning Contributivity.\n",
    "\n",
    "This is based on both [existing resources on MNIST](https://medium.com/@mjbhobe/mnist-digits-classification-with-keras-ed6c2374bd0e) and [precedent implementation of this dataset for the standalone application](https://github.com/SubstraFoundation/distributed-learning-contributivity/blob/master/datasets/dataset_mnist.py).\n",
    "\n",
    "This notebook will be focused on importing manually the dataset, do a bit of preprocessing and build our objects to run a collaborative round.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "In order to run this example, you'll need to:\n",
    "\n",
    "* use python 3.7 +\n",
    "* install requierements from the requirements.txt file\n",
    "* install this package https://test.pypi.org/project/pkg-test-distributed-learning-contributivity/0.0.7/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NXZm5jUGdpt6",
    "outputId": "ef7e58cb-7acc-45a9-c068-00ba8fd8d2b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex‚cutable ou un fichier de commandes.\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0 (from subtest==0.0.0.10) (from versions: none)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://test.pypi.org/simple/\n",
      "Collecting subtest==0.0.0.10\n",
      "  Downloading https://test-files.pythonhosted.org/packages/b8/5a/2540de2b5d53a93ddb6513c0aed65aef8e964b7d9d951011055bcc55fa61/subtest-0.0.0.10-py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: librosa==0.8.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.8.0)\n",
      "Requirement already satisfied: scikit-learn==0.22.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.22.1)\n",
      "Requirement already satisfied: loguru==0.4.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR: No matching distribution found for tensorflow==2.2.0 (from subtest==0.0.0.10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ruamel.yaml==0.16.10 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.16.10)\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/SubstraFoundation/distributed-learning-contributivity/Moving-functions/requirements.txt\n",
    "!pip install -r requirements.txt\n",
    "!pip install -i https://test.pypi.org/simple/ subtest==0.0.0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://test.pypi.org/simple/\n",
      "Collecting subtest==0.0.0.10\n",
      "  Downloading https://test-files.pythonhosted.org/packages/b8/5a/2540de2b5d53a93ddb6513c0aed65aef8e964b7d9d951011055bcc55fa61/subtest-0.0.0.10-py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: matplotlib==3.1.3 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (3.1.3)\n",
      "Requirement already satisfied: pandas==1.0.5 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (1.0.5)\n",
      "Requirement already satisfied: loguru==0.4.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.4.1)\n",
      "Requirement already satisfied: flake8==3.8.3 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (3.8.3)\n",
      "Requirement already satisfied: seaborn==0.10.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.10.0)\n",
      "Requirement already satisfied: scikit-learn==0.22.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.22.1)\n",
      "Requirement already satisfied: librosa==0.8.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.8.0)\n",
      "Requirement already satisfied: tensorflow==2.2.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (2.2.0)\n",
      "Requirement already satisfied: ruamel.yaml==0.16.10 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (0.16.10)\n",
      "Requirement already satisfied: pytest==5.4.3 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (5.4.3)\n",
      "Requirement already satisfied: scipy==1.4.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (1.4.1)\n",
      "Requirement already satisfied: Keras==2.3.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (2.3.1)\n",
      "Requirement already satisfied: numpy==1.19.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from subtest==0.0.0.10) (1.19.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from matplotlib==3.1.3->subtest==0.0.0.10) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from matplotlib==3.1.3->subtest==0.0.0.10) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from matplotlib==3.1.3->subtest==0.0.0.10) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from matplotlib==3.1.3->subtest==0.0.0.10) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from pandas==1.0.5->subtest==0.0.0.10) (2020.1)\n",
      "Requirement already satisfied: colorama>=0.3.4; sys_platform == \"win32\" in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from loguru==0.4.1->subtest==0.0.0.10) (0.4.3)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0; sys_platform == \"win32\" in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from loguru==0.4.1->subtest==0.0.0.10) (1.0.1)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from flake8==3.8.3->subtest==0.0.0.10) (0.6.1)\n",
      "Requirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from flake8==3.8.3->subtest==0.0.0.10) (2.6.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from flake8==3.8.3->subtest==0.0.0.10) (1.6.0)\n",
      "Requirement already satisfied: pyflakes<2.3.0,>=2.2.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from flake8==3.8.3->subtest==0.0.0.10) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from scikit-learn==0.22.1->subtest==0.0.0.10) (0.15.1)\n",
      "Requirement already satisfied: soundfile>=0.9.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from librosa==0.8.0->subtest==0.0.0.10) (0.10.3.post1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from librosa==0.8.0->subtest==0.0.0.10) (0.2.2)\n",
      "Requirement already satisfied: decorator>=3.0.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from librosa==0.8.0->subtest==0.0.0.10) (4.4.2)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from librosa==0.8.0->subtest==0.0.0.10) (1.1.1)\n",
      "Requirement already satisfied: numba>=0.43.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from librosa==0.8.0->subtest==0.0.0.10) (0.51.2)\n",
      "Requirement already satisfied: audioread>=2.0.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from librosa==0.8.0->subtest==0.0.0.10) (2.1.8)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (1.27.2)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (3.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (1.15.0)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (2.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (1.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (0.9.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (2.2.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (0.35.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (0.3.3)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorflow==2.2.0->subtest==0.0.0.10) (2.10.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\" in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from ruamel.yaml==0.16.10->subtest==0.0.0.10) (0.2.0)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from pytest==5.4.3->subtest==0.0.0.10) (1.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from pytest==5.4.3->subtest==0.0.0.10) (20.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from pytest==5.4.3->subtest==0.0.0.10) (19.3.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from pytest==5.4.3->subtest==0.0.0.10) (8.5.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from pytest==5.4.3->subtest==0.0.0.10) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from pytest==5.4.3->subtest==0.0.0.10) (0.1.9)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from pytest==5.4.3->subtest==0.0.0.10) (1.4.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from Keras==2.3.1->subtest==0.0.0.10) (5.3.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from Keras==2.3.1->subtest==0.0.0.10) (1.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->flake8==3.8.3->subtest==0.0.0.10) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from soundfile>=0.9.0->librosa==0.8.0->subtest==0.0.0.10) (1.14.0)\n",
      "Requirement already satisfied: requests in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from pooch>=1.0->librosa==0.8.0->subtest==0.0.0.10) (2.23.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from pooch>=1.0->librosa==0.8.0->subtest==0.0.0.10) (1.4.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from numba>=0.43.0->librosa==0.8.0->subtest==0.0.0.10) (50.3.0)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from numba>=0.43.0->librosa==0.8.0->subtest==0.0.0.10) (0.34.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->subtest==0.0.0.10) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->subtest==0.0.0.10) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->subtest==0.0.0.10) (0.16.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->subtest==0.0.0.10) (1.14.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->subtest==0.0.0.10) (1.7.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.8.0->subtest==0.0.0.10) (2.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from requests->pooch>=1.0->librosa==0.8.0->subtest==0.0.0.10) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from requests->pooch>=1.0->librosa==0.8.0->subtest==0.0.0.10) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from requests->pooch>=1.0->librosa==0.8.0->subtest==0.0.0.10) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from requests->pooch>=1.0->librosa==0.8.0->subtest==0.0.0.10) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->subtest==0.0.0.10) (1.2.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->subtest==0.0.0.10) (4.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->subtest==0.0.0.10) (0.2.7)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->subtest==0.0.0.10) (4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->subtest==0.0.0.10) (3.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\arthu\\anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->subtest==0.0.0.10) (0.4.8)\n",
      "Installing collected packages: subtest\n",
      "Successfully installed subtest-0.0.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://test.pypi.org/simple/ subtest==0.0.0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall subtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur D s'appelle DATA\n",
      " Le num‚ro de s‚rie du volume est 1EED-98E0\n",
      "\n",
      " R‚pertoire de D:\\Documents\\Substra\\repos\\mplc\\distributed-learning-contributivity\n",
      "\n",
      "16/09/2020  10:14    <DIR>          .\n",
      "16/09/2020  10:14    <DIR>          ..\n",
      "03/09/2020  10:45    <DIR>          .github\n",
      "14/09/2020  12:13             1ÿ497 .gitignore\n",
      "03/09/2020  10:45                 0 .gitmodules\n",
      "16/09/2020  10:52    <DIR>          .idea\n",
      "14/09/2020  15:55    <DIR>          .pytest_cache\n",
      "14/09/2020  11:47               303 .travis.yml\n",
      "11/09/2020  11:33    <DIR>          __pycache__\n",
      "14/09/2020  19:36    <DIR>          build\n",
      "14/09/2020  10:39             1ÿ261 config.yml\n",
      "14/09/2020  12:45               629 config_quick_debug.yml\n",
      "03/09/2020  10:45             7ÿ168 CONTRIBUTING.md\n",
      "16/09/2020  10:02    <DIR>          dist\n",
      "16/09/2020  10:28    <DIR>          experiments\n",
      "03/09/2020  10:45    <DIR>          img\n",
      "03/09/2020  10:45            11ÿ558 LICENSE\n",
      "15/09/2020  19:14             6ÿ283 main.py\n",
      "14/09/2020  11:43    <DIR>          notebooks\n",
      "15/09/2020  19:19            27ÿ769 README.md\n",
      "16/09/2020  10:14               203 requirements.txt\n",
      "03/09/2020  10:45    <DIR>          saved_experiments\n",
      "16/09/2020  10:00             1ÿ014 setup.py\n",
      "14/09/2020  10:39    <DIR>          shapley_value\n",
      "15/09/2020  19:19    <DIR>          subtest\n",
      "16/09/2020  10:02    <DIR>          subtest.egg-info\n",
      "15/09/2020  19:14    <DIR>          tests\n",
      "              11 fichier(s)           57ÿ685 octets\n",
      "              16 R‚p(s)  862ÿ314ÿ041ÿ344 octets libres\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "Dv0WF-jEddPL",
    "outputId": "ad7b4fdc-68c4-4f38-88a5-7e018cc04273"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "r8ZI1KswddPN",
    "outputId": "8d8907cf-4b08-47f6-a6f2-b96df350658f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\arthu\\Anaconda3\\envs\\distributed-learning-contributivity\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Object and methodes needed in order to run a collaborative round\n",
    "from subtest.datasets.dataset import Dataset\n",
    "from subtest.scenario import Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrYrGPY1ddPP"
   },
   "source": [
    "# Create a custom scenario handling mandatory parameters\n",
    "\n",
    "These parameters describe how many partners will be created and how much proportion they will have in the dataset.\n",
    "\n",
    "We can use more advanced sample split options in order to fine tune the data distribution between partners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qD3t3vT3ddPS"
   },
   "source": [
    "# Set values for scenario parameters\n",
    "\n",
    "## Mandatory parameters\n",
    "partner_count and amounts_per_partner describe how many partners will be created and how much proportion they will have in the dataset. Here we choose 4 partners, with respectively 20 %, 50% and 30% of the dataset. \n",
    "\n",
    "We can use more advanced sample split options in order to fine tune the data distribution between partners.\n",
    "\n",
    "## Optionnal parameters\n",
    "\n",
    "We want our training to go for 10 epochs and 3 minibatches per epoch.\n",
    "\n",
    "Moreover, there is 4 datasets which are pre-implemented in subtest, but for the example we will create our own dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2s7grIQmddPX"
   },
   "source": [
    "# Create Data Set\n",
    "\n",
    "For this experiment we use the well known MNIST dataset.\n",
    "\n",
    "This example is also available using the standalone app specifying in the config file : dataset_name: - 'mnist', or by passing the parameter dataset_name = 'mnist' to Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZMKAQispddPX"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],  28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3CfWhs8ddPZ"
   },
   "source": [
    "# Create Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtjWVBXAddPa"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset_labels(y):\n",
    "    y = np_utils.to_categorical(y, 10)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-Ygu456ddPe"
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxHt1CUAddPe"
   },
   "outputs": [],
   "source": [
    "def generate_new_model_for_dataset():\n",
    "    model = Sequential()\n",
    "    # add Convolutional layers\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))    \n",
    "    model.add(Flatten())\n",
    "    # Densely connected layers\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # compile with adam optimizer & categorical_crossentropy loss function\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1e-21brZddPg"
   },
   "source": [
    "# Generate dataset\n",
    "\n",
    "Note that the scenario needs a dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    \"my_dataset\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    input_shape,\n",
    "    num_classes,\n",
    "    preprocess_dataset_labels,\n",
    "    generate_new_model_for_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'train_val_split_local'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-e0cfd64da5f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_val_split_local\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'train_val_split_local'"
     ]
    }
   ],
   "source": [
    "dataset.train_val_split_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CztnBVbPddPU"
   },
   "source": [
    "#### Every other parameter will be set to its default value\n",
    "\n",
    "We might consider :\n",
    "\n",
    "- Datas will be split randomly between partner\n",
    "- The learning approach is 'fedavg' for federated averaging \n",
    "- Weights will be averaged uniformly, different weights can be applied for each partner\n",
    "\n",
    "The learning approaches are built-in paramater that can be set easily. There are currently 4 differents approaches.\n",
    "\n",
    "\n",
    "#### More details at : https://github.com/SubstraFoundation/distributed-learning-contributivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaTh2os1ddPU"
   },
   "source": [
    "# Define scenario\n",
    "\n",
    "We specify our experiment path used to output graphs and results.\n",
    "\n",
    "We can now create the scenario that will handle every parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "7ZU1TCA2ddPV",
    "outputId": "0180cd4d-b108-44db-e24b-d366baa6297d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-16 10:28:08.235 | DEBUG    | subtest.scenario:__init__:101 - Computation use the full dataset for scenario #1\n",
      "2020-09-16 10:28:08.579 | INFO     | subtest.scenario:__init__:262 - ### Description of data scenario configured:\n",
      "2020-09-16 10:28:08.580 | INFO     | subtest.scenario:__init__:263 -    Number of partners defined: 3\n",
      "2020-09-16 10:28:08.581 | INFO     | subtest.scenario:__init__:264 -    Data distribution scenario chosen: random\n",
      "2020-09-16 10:28:08.581 | INFO     | subtest.scenario:__init__:265 -    Multi-partner learning approach: fedavg\n",
      "2020-09-16 10:28:08.582 | INFO     | subtest.scenario:__init__:266 -    Weighting option: uniform\n",
      "2020-09-16 10:28:08.583 | INFO     | subtest.scenario:__init__:267 -    Iterations parameters: 10 epochs > 3 mini-batches > 8 gradient updates per pass\n",
      "2020-09-16 10:28:08.584 | INFO     | subtest.scenario:__init__:273 - ### Data loaded: my_dataset\n",
      "2020-09-16 10:28:08.585 | INFO     | subtest.scenario:__init__:274 -    54000 train data with 54000 labels\n",
      "2020-09-16 10:28:08.586 | INFO     | subtest.scenario:__init__:275 -    6000 val data with 6000 labels\n",
      "2020-09-16 10:28:08.587 | INFO     | subtest.scenario:__init__:276 -    10000 test data with 10000 labels\n"
     ]
    }
   ],
   "source": [
    "current_scenario = Scenario(partners_count = 3,\n",
    "                            amounts_per_partner = [0.2, 0.5, 0.3],\n",
    "                            epoch_count = 10,\n",
    "                            minibatch_count = 3,\n",
    "                            dataset = dataset\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVLFZNorddPk"
   },
   "source": [
    "# Run scenario\n",
    "\n",
    "The actual training phase of our federated learning example ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5HA_xIvsddPl",
    "outputId": "f0d9b4eb-2385-48e2-87c5-a058053d0fad"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'train_val_split_local'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0b97c1569165>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcurrent_scenario\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Documents\\Substra\\repos\\mplc\\distributed-learning-contributivity\\subtest\\scenario.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;31m# ... train data, early stopping validation data, test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples_split_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'basic'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples_split_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'advanced'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_data_advanced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\Substra\\repos\\mplc\\distributed-learning-contributivity\\subtest\\scenario.py\u001b[0m in \u001b[0;36msplit_data\u001b[1;34m(self, is_logging_enabled)\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[1;31m# Create local validation and test datasets from the partner train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_test_split_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_val_split_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m             \u001b[1;31m# Update other attributes from partner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'train_val_split_local'"
     ]
    }
   ],
   "source": [
    "current_scenario.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeWcWlC5ddPn"
   },
   "source": [
    "# Results\n",
    "\n",
    "We can see every parameter used pre and post training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "X67YaGm3ddPn",
    "outputId": "5a8ba01f-ef5f-426f-ec7a-5c07471f6245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['aggregation_weighting', 'dataset_fraction_per_partner', 'dataset_name',\n",
      "       'epoch_count', 'final_relative_nb_samples',\n",
      "       'gradient_updates_per_pass_count', 'is_early_stopping',\n",
      "       'learning_computation_time_sec', 'minibatch_count',\n",
      "       'mpl_nb_epochs_done', 'mpl_test_score',\n",
      "       'multi_partner_learning_approach', 'nb_samples_used', 'partners_count',\n",
      "       'samples_split_description', 'scenario_name', 'short_scenario_name',\n",
      "       'test_data_samples_count', 'train_data_samples_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_results = current_scenario.to_dataframe()\n",
    "print(df_results.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XW_EH1fxddPp"
   },
   "source": [
    "#### Our score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "hUBKGXfXddPp",
    "outputId": "c8d03c0d-1501-4d3b-96c5-e1e29fd4e49b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach used : fedavg\n",
      "Model accuracy : 0.9811000227928162\n",
      "0    uniform\n",
      "Name: aggregation_weighting, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Approach used :\", df_results.multi_partner_learning_approach[0])\n",
    "print(\"Model accuracy :\", df_results.mpl_test_score[0])\n",
    "print(df_results.aggregation_weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99jaKK2LddPr"
   },
   "source": [
    "## Extract model \n",
    "\n",
    "We can extract our model and save it for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9wIdoS_bddPr"
   },
   "outputs": [],
   "source": [
    "model = current_scenario.mpl.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "M4azOyxqddPu",
    "outputId": "3a4a8904-f156-4e96-b29b-1ff5c7faa60b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 65us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05563341381018981, 0.9811000227928162]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, preprocess_dataset_labels(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VheE9NyxddPw"
   },
   "source": [
    "# That's it !\n",
    "\n",
    "Now you can explore our other tutorials for a better snapshot of what can be done with our library!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "1 _INTRO_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = pd.read_csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv', index_col=False )\n",
    "\n",
    "raw_dataset['Fam_size'] = raw_dataset['Siblings/Spouses Aboard'] + raw_dataset['Parents/Children Aboard']\n",
    "\n",
    "raw_dataset['Name_Len'] = [ len(i) for i in raw_dataset[\"Name\"] ]\n",
    "\n",
    "raw_dataset['Is_alone'] = [ i == 0 for i in raw_dataset[\"Fam_size\"] ]\n",
    "\n",
    "raw_dataset[\"Sex\"] = [ i == \"Male\" for i in raw_dataset[\"Sex\"] ]\n",
    "\n",
    "raw_dataset['Title'] = [ i.split()[0] for i in raw_dataset[\"Name\"]]\n",
    "raw_dataset = pd.concat([raw_dataset, pd.get_dummies(raw_dataset['Title'])], axis=1)\n",
    "\n",
    "raw_dataset = pd.concat([raw_dataset, pd.get_dummies(raw_dataset['Pclass'])], axis=1)\n",
    "\n",
    "# Dropping the useless features\n",
    "raw_dataset.drop('Name', axis=1, inplace=True)\n",
    "raw_dataset.drop('Pclass', axis=1, inplace=True)\n",
    "raw_dataset.drop('Siblings/Spouses Aboard', axis=1, inplace=True)\n",
    "raw_dataset.drop('Parents/Children Aboard', axis=1, inplace=True)\n",
    "raw_dataset.drop('Title', axis=1, inplace=True)\n",
    "\n",
    "x = raw_dataset.drop('Survived', axis=1).to_numpy()\n",
    "y = raw_dataset['Survived'].to_numpy()\n",
    "\n",
    "\n",
    "#prepare dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug : Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=10000, warm_start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche naive :\n",
    "On incrémente au fil du jeu de donnée pour voir si on obtient un résultat similaire à si on avait tout fait d'un coup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itérrations : 29\n",
      "itération 0 : 0.7064846416382252\n",
      "itération 1 : 0.7030716723549488\n",
      "itération 2 : 0.7201365187713311\n",
      "itération 3 : 0.6723549488054608\n",
      "itération 4 : 0.6313993174061433\n",
      "itération 5 : 0.7508532423208191\n",
      "itération 6 : 0.7098976109215017\n",
      "itération 7 : 0.658703071672355\n",
      "itération 8 : 0.726962457337884\n",
      "itération 9 : 0.7337883959044369\n",
      "itération 10 : 0.7508532423208191\n",
      "itération 11 : 0.5665529010238908\n",
      "itération 12 : 0.757679180887372\n",
      "itération 13 : 0.6484641638225256\n",
      "itération 14 : 0.689419795221843\n",
      "itération 15 : 0.7610921501706485\n",
      "itération 16 : 0.6860068259385665\n",
      "itération 17 : 0.7713310580204779\n",
      "itération 18 : 0.7167235494880546\n",
      "itération 19 : 0.6928327645051194\n",
      "itération 20 : 0.6655290102389079\n",
      "itération 21 : 0.7440273037542662\n",
      "itération 22 : 0.7133105802047781\n",
      "itération 23 : 0.6313993174061433\n",
      "itération 24 : 0.6552901023890785\n",
      "itération 25 : 0.7337883959044369\n",
      "itération 26 : 0.78839590443686\n",
      "itération 27 : 0.6621160409556314\n",
      "itération 28 : 0.6791808873720137\n",
      "-------------\n",
      "entrainement sur tout le jeu de donnée d'un coup : 0.7849829351535836\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "iter = math.floor(len(x_train)/batch_size)\n",
    "\n",
    "print('itérrations :', iter)\n",
    "\n",
    "for i in range(iter):\n",
    "    x = x_train[batch_size*i : batch_size*(i+1)]\n",
    "    y = y_train[batch_size*i : batch_size*(i+1)]\n",
    "    clf.fit(x, y)\n",
    "    print(\"itération\", i, \":\", clf.score(x_test, y_test))\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "x = x_train[0 : iter*batch_size]\n",
    "y = y_train[0 : iter*batch_size]\n",
    "clf.fit(x, y)\n",
    "print(\"entrainement sur tout le jeu de donnée d'un coup :\",clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats :\n",
    "\n",
    "ca n'a pas l'air de fonctionner, on a l'impression que le classifier est réinitialisé et réentrainé à chaque mini batch\n",
    "\n",
    "notamment sur ces 3 lignes :\n",
    "```\n",
    "itération 10 : 0.7508532423208191\n",
    "itération 11 : 0.5665529010238908\n",
    "itération 12 : 0.757679180887372\n",
    "```\n",
    "De plus, sur la dernière ittération, les performances semblent loin d'être optimales comparées à l'apprentissage sur l'intégralité du jeu de donnée :\n",
    "```\n",
    "itération 28 : 0.6791808873720137\n",
    "```\n",
    "\n",
    "```\n",
    "entrainement sur tout le jeu de donnée d'un coup : 0.7849829351535836\n",
    "```\n",
    "\n",
    "Explication possible : Peut-être que sur l'ittération 11 on a affaire a un sous-échantillon qui dévie fortement du jeu de test ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation :\n",
    "\n",
    "Essayons de vérifier les poids entre deux \"epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itérrations : 29\n",
      "nombre d'itérations avant de converger : [108]\n",
      "minibatch 0 : 0.7064846416382252\n",
      "nombre d'itérations avant de converger : [154]\n",
      "minibatch 1 : 0.7030716723549488\n",
      "nombre d'itérations avant de converger : [146]\n",
      "minibatch 2 : 0.7201365187713311\n",
      "nombre d'itérations avant de converger : [114]\n",
      "minibatch 3 : 0.6723549488054608\n",
      "nombre d'itérations avant de converger : [167]\n",
      "minibatch 4 : 0.6313993174061433\n",
      "-------------\n",
      "nombre d'itérations avant de converger : [284]\n",
      "entrainement sur tout le jeu de donnée d'un coup : 0.7406143344709898\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=10000, warm_start=1, random_state = 0)\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "iter = math.floor(len(x_train)/batch_size)\n",
    "\n",
    "fewer_iter = 5\n",
    "\n",
    "print('itérrations :', iter)\n",
    "\n",
    "for i in range(fewer_iter):\n",
    "    x = x_train[batch_size*i : batch_size*(i+1)]\n",
    "    y = y_train[batch_size*i : batch_size*(i+1)]\n",
    "    clf.fit(x, y)\n",
    "    print(\"nombre d'itérations avant de converger :\",clf.n_iter_)\n",
    "    print(\"minibatch\", i, \":\", clf.score(x_test, y_test))\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "x = x_train[0 : fewer_iter*batch_size]\n",
    "y = y_train[0 : fewer_iter*batch_size]\n",
    "clf.fit(x, y)\n",
    "print(\"nombre d'itérations avant de converger :\",clf.n_iter_)\n",
    "print(\"entrainement sur tout le jeu de donnée d'un coup :\", clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ci dessus, une cellule ayant un comportement qui, je pensais, devait être similaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itérrations : 29\n",
      "nombre d'itérations avant de converger : [108]\n",
      "minibatch 0 : 0.7064846416382252\n",
      "nombre d'itérations avant de converger : [147]\n",
      "minibatch 1 : 0.7098976109215017\n",
      "nombre d'itérations avant de converger : [194]\n",
      "minibatch 2 : 0.7201365187713311\n",
      "nombre d'itérations avant de converger : [193]\n",
      "minibatch 3 : 0.7201365187713311\n",
      "nombre d'itérations avant de converger : [150]\n",
      "minibatch 4 : 0.7440273037542662\n",
      "-------------\n",
      "nombre d'itérations avant de converger : [1]\n",
      "entrainement sur tout le jeu de donnée d'un coup : 0.7440273037542662\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=10000, warm_start=1, random_state=0)\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "iter = math.floor(len(x_train)/batch_size)\n",
    "fewer_iter = 5\n",
    "\n",
    "print('itérrations :', iter)\n",
    "\n",
    "for i in range(fewer_iter):\n",
    "    x = x_train[0 : batch_size*(i+1)]\n",
    "    y = y_train[0 : batch_size*(i+1)]\n",
    "    clf.fit(x, y)\n",
    "    print(\"nombre d'itérations avant de converger :\",clf.n_iter_)\n",
    "    print(\"minibatch\", i, \":\", clf.score(x_test, y_test))\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "x = x_train[0 : fewer_iter*batch_size]\n",
    "y = y_train[0 : fewer_iter*batch_size]\n",
    "clf.fit(x, y)\n",
    "print(\"nombre d'itérations avant de converger :\",clf.n_iter_)\n",
    "print(\"entrainement sur tout le jeu de donnée d'un coup :\", clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats\n",
    "\n",
    "Quand on regarde le nombre d'itérations avant de converger dans les deux experiences on remarque plusieurs choses.\n",
    "\n",
    "A chaque minibatch, l'algoritme met un certain temps avant de converger. On en conclut donc que d'un minibatch à l'autre, il y a bien un nouveau minimum a chercher.\n",
    "\n",
    "Dans le cas des entraitement sur le dataset complet, on a deux cas différents :\n",
    "\n",
    "Dans le premiers cas, on se situait déjà sur le minimum du dernier minibatch, il y a donc ne nombreuses étapes avant d'arriver sur le minimum global.\n",
    "\n",
    "Dans le deuxieme cas, on se situe déjà sur le minimum global, il n'y a donc qu'un pas. Cela confirme le fait que rappeller la methode fit() ne réinitialise pas le modele entre deux époch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essayons de réinitialiser le model pour voir l'impact sur le nombre d'itération entre chaque minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itérrations : 29\n",
      "nombre d'itérations avant de converger : [108]\n",
      "minibatch 0 : 0.7064846416382252\n",
      "nombre d'itérations avant de converger : [171]\n",
      "minibatch 1 : 0.7098976109215017\n",
      "nombre d'itérations avant de converger : [203]\n",
      "minibatch 2 : 0.7201365187713311\n",
      "nombre d'itérations avant de converger : [225]\n",
      "minibatch 3 : 0.7201365187713311\n",
      "nombre d'itérations avant de converger : [236]\n",
      "minibatch 4 : 0.7440273037542662\n",
      "-------------\n",
      "nombre d'itérations avant de converger : [1]\n",
      "entrainement sur tout le jeu de donnée d'un coup : 0.7440273037542662\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "iter = math.floor(len(x_train)/batch_size)\n",
    "fewer_iter = 5\n",
    "\n",
    "print('itérrations :', iter)\n",
    "\n",
    "for i in range(fewer_iter):\n",
    "    clf = LogisticRegression(max_iter=10000, warm_start=True, random_state = 0)\n",
    "    x = x_train[0 : batch_size*(i+1)]\n",
    "    y = y_train[0 : batch_size*(i+1)]\n",
    "    clf.fit(x, y)\n",
    "    print(\"nombre d'itérations avant de converger :\",clf.n_iter_)\n",
    "    print(\"minibatch\", i, \":\", clf.score(x_test, y_test))\n",
    "    \n",
    "print(\"-------------\")\n",
    "x = x_train[0 : fewer_iter*batch_size]\n",
    "y = y_train[0 : fewer_iter*batch_size]\n",
    "clf.fit(x, y)\n",
    "print(\"nombre d'itérations avant de converger :\",clf.n_iter_)\n",
    "print(\"entrainement sur tout le jeu de donnée d'un coup :\", clf.score(x_test, y_test))\n",
    "print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate une légère dégradation. \n",
    "\n",
    "Ce qui semble confirmer que les poids n'étaient pas réinitialiser à chaque fit() successif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inutile de faire du séquentiel pure ?\n",
    "\n",
    "Il semblerait que cette approche n'ai pas d'interet. \n",
    "Ou au moins qu'elle n'ai pas d'interet sur ce jeu de données.\n",
    "\n",
    "A chaque minibatch, l'algoritme converge vers une solution \"optimale\" (pour ce sub-dataset précis). \n",
    "\n",
    "En partant de cette nouvelle solution, le model va chercher à converger vers un nouveau minimum qui est celui du mimibatch actuel.\n",
    "\n",
    "En séquentiel, les minimum correspondent a des jeux de données plus restreints qui ne peuvent être que moins optimaux que le minimum atteint par le dateset complet pour un probleme donné.\n",
    "\n",
    "Cette approche n'a donc aucun interet puisqu'elle converge vers le minimum de chaque minibatch, et que le minimum global ne peut pas être atteint.\n",
    "\n",
    "## Perspectives à explorer :\n",
    "\n",
    "On pourrait artificiellement limiter max_iter pour empecher le modele de converger à chaque fois.\n",
    "\n",
    "Ainsi, on pourrait voir si cette approche peut présenter un interet dans le cas ou l'algoritme n'arrive pas à converger.\n",
    "\n",
    "Cela donnerais des indice sur l'éventuelle utilité d'une approche séquentielle sur des problemes ou des modeles plus compliqués."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche en federated averaging\n",
    "\n",
    "Chaque partenaire entraine son modele sur un minibatch différent en paralele, on moyene les poids, on test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de partenairs : 4\n",
      "itération 0 : 0.7781569965870307\n",
      "itération 1 : 0.764505119453925\n",
      "itération 2 : 0.7815699658703071\n",
      "itération 3 : 0.7952218430034129\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=10000, random_state = 0)\n",
    "\n",
    "\n",
    "n_partner = 4\n",
    "batch_size = math.floor(len(x_train)/n_partner)\n",
    "\n",
    "print('nombre de partenairs :', n_partner)\n",
    "\n",
    "#initialisation de la matrice de poids\n",
    "coefs = [None]* n_partner\n",
    "intercepts = [None]* n_partner\n",
    "\n",
    "for i in range(n_partner):\n",
    "    x = x_train[batch_size*i : batch_size*(i+1)]\n",
    "    y = y_train[batch_size*i : batch_size*(i+1)]\n",
    "    clf.fit(x, y)\n",
    "    (coefs[i], intercepts[i]) = (clf.coef_,clf.intercept_)\n",
    "    print(\"itération\", i, \":\", clf.score(x_test, y_test))\n",
    "\n",
    "\n",
    "sum_coefs = coefs[0]*0\n",
    "sum_intercepts = intercepts[0]*0\n",
    "\n",
    "for i in range(n_partner):\n",
    "    sum_coefs = sum_coefs + coefs[i]\n",
    "    sum_intercepts = intercepts + intercepts[i]\n",
    "        \n",
    "    \n",
    "#averaging\n",
    "avg_coef = sum_coefs/n_partner\n",
    "avg_intercepts = sum_intercepts/n_partner\n",
    "\n",
    "# assemblage du model moyenné :\n",
    "clf = LogisticRegression()\n",
    "\n",
    "\n",
    "clf.coef_ = avg_coef\n",
    "clf.intercept_ = avg_intercepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test du modele moyenné et comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True]] [[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (293,1) (4,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-303-dbf12d017109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test sur modele moyenné :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    367\u001b[0m         \"\"\"\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n\u001b[1;32m--> 276\u001b[1;33m                                  dense_output=True) + self.intercept_\n\u001b[0m\u001b[0;32m    277\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (293,1) (4,1) "
     ]
    }
   ],
   "source": [
    "print(clf.coef_ == avg_coef, clf.intercept_ == avg_intercepts)\n",
    "\n",
    "\n",
    "print(\"test sur modele moyenné :\", clf.score(x_test, y_test))\n",
    "\n",
    "print(\"-------------\")\n",
    "x = x_train[0 : batch_size*n_partner]\n",
    "y = y_train[0 : batch_size*n_partner]\n",
    "clf = LogisticRegression(max_iter=10000, random_state = 0)\n",
    "\n",
    "clf.fit(x, y)\n",
    "print(\"test sur tout le jeu de donnée d'un coup :\", clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "0\n",
      "0.4641638225255973\n",
      "1\n",
      "0.4334470989761092\n",
      "2\n",
      "0.6757679180887372\n",
      "3\n",
      "0.6109215017064846\n",
      "4\n",
      "0.6825938566552902\n",
      "5\n",
      "0.6825938566552902\n",
      "6\n",
      "0.6040955631399317\n",
      "7\n",
      "0.3890784982935154\n",
      "8\n",
      "0.45051194539249145\n",
      "9\n",
      "0.6996587030716723\n",
      "10\n",
      "0.3993174061433447\n",
      "11\n",
      "0.5085324232081911\n",
      "12\n",
      "0.6143344709897611\n",
      "13\n",
      "0.7030716723549488\n",
      "14\n",
      "0.6143344709897611\n",
      "15\n",
      "0.6109215017064846\n",
      "16\n",
      "0.5767918088737202\n",
      "17\n",
      "0.6518771331058021\n",
      "18\n",
      "0.6109215017064846\n",
      "19\n",
      "0.6109215017064846\n",
      "20\n",
      "0.7030716723549488\n",
      "21\n",
      "0.6791808873720137\n",
      "22\n",
      "0.44368600682593856\n",
      "23\n",
      "0.6109215017064846\n",
      "24\n",
      "0.3890784982935154\n",
      "25\n",
      "0.6518771331058021\n",
      "26\n",
      "0.6143344709897611\n",
      "27\n",
      "0.45051194539249145\n",
      "28\n",
      "0.41638225255972694\n",
      "29\n",
      "0.6450511945392492\n",
      "30\n",
      "0.6450511945392492\n",
      "31\n",
      "0.6279863481228669\n",
      "32\n",
      "0.6928327645051194\n",
      "33\n",
      "0.6860068259385665\n",
      "34\n",
      "0.6006825938566553\n",
      "35\n",
      "0.689419795221843\n",
      "36\n",
      "0.6450511945392492\n",
      "37\n",
      "0.6484641638225256\n",
      "38\n",
      "0.6416382252559727\n",
      "39\n",
      "0.6689419795221843\n",
      "40\n",
      "0.6313993174061433\n",
      "41\n",
      "0.6825938566552902\n",
      "42\n",
      "0.3583617747440273\n",
      "43\n",
      "0.658703071672355\n",
      "44\n",
      "0.3924914675767918\n",
      "45\n",
      "0.6143344709897611\n",
      "46\n",
      "0.6109215017064846\n",
      "47\n",
      "0.5631399317406144\n",
      "48\n",
      "0.6757679180887372\n",
      "49\n",
      "0.590443686006826\n",
      "50\n",
      "0.6723549488054608\n",
      "51\n",
      "0.5119453924914675\n",
      "52\n",
      "0.6655290102389079\n",
      "53\n",
      "0.689419795221843\n",
      "54\n",
      "0.689419795221843\n",
      "55\n",
      "0.45051194539249145\n",
      "56\n",
      "0.6791808873720137\n",
      "57\n",
      "0.6962457337883959\n",
      "58\n",
      "0.6689419795221843\n",
      "-------------\n",
      "0.726962457337884\n"
     ]
    }
   ],
   "source": [
    "iter = math.floor(len(x_train)/10)\n",
    "\n",
    "print(iter)\n",
    "\n",
    "for i in range(iter):\n",
    "    print(i)\n",
    "    x = x_train[10*i:10*(i+1)]\n",
    "    y = y_train[10*i:10*(i+1)]\n",
    "    clf.partial_fit(x, y, classes=[0,1])\n",
    "    print(clf.score(x_test, y_test))\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "x = x_train[0:iter*10]\n",
    "y = y_train[0:iter*10]\n",
    "clf.fit(x, y)\n",
    "print(clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7849829351535836\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
